<!DOCTYPE html>
<html class="no-js" lang="en-US">
<head>
  <meta charset="utf-8">
  <title>1st International Workshop on Multiscale Multimodal Medical Imaging (MMMI 2019)</title>
  <meta http-equiv="Content-Type" content="application/xhtml+xml; charset=utf-8" />
	<meta name="description" content="Homepage of the 1st International Workshop on Multiscale Multimodal Medical Imaging (MMMI 2019)" />
  <link rel="stylesheet" type="text/css" href="./screen.css" media="screen" />
  <link rel="stylesheet" href="./w3.css">
  <link  rel="stylesheet" href="./googlefonts.css">	
</head>
<body>
<div style="width: 60%; float:left">
	<h2><strong>1st International Workshop on </strong></h2>
	<h2><strong>Multiscale Multimodal Medical Imaging (MMMI 2019)</strong></h2>
  <h4>In conjunction with <a href="https://www.miccai2019.org/">22nd International Conference on Medical Image Computing and Computer Assisted Intervention</a>, October 13, 2019, Shenzhen, China</h4>
</div>
<div style="width: 40%; float:right">
	<a href="http://www.miccai.org/"><img src="./MICCAI-Logo.jpg" alt="" height="50" /></a>
	<a href="http://english.pku.edu.cn/"><img src="./PKU-Logo.png" alt="" height="50" /></a>
  	<a href="https://viterbischool.usc.edu/"><img src="./USC-Logo.png" alt="" height="50" /></a><br />
	<a href="https://www.massgeneral.org/Imaging/"><img src="./MGH-Logo.jpg" alt="" height="45" /></a>
	<a href="https://hms.harvard.edu/"><img src="./HMS-Logo.png" alt="" height="45" /></a>
  
</div>

<div class="w3-bar w3-black">
  <button class="w3-bar-item w3-button" onclick="openTab('Home')">Home</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Schedule')">Schedule</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Submission')">Submission</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Organization')">Organization</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Dates')">Important Dates</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Registration')">Registration</button>
</div>

<div id="Home" class="w3-container tab">
  <h2>Update</h2>
  <p>Proceeding of the workshop, as part of the MICCAI 2019 conference proceeding, has been published as <strong>Lecture Notes in Computer Science (LNCS)</strong> book series. The proceeding is now available online at <a href="https://www.springer.com/us/book/9783030379681">Springer</a></p>
  <p>Inivtation letter for MMMI'19 is now available for downloading <a href="./invitationLetter_MMMI19.docx">here</a></p>
  <p>Workshop schedule is now <strong>online</strong>. This year we have the pleasure of inviting <strong>Prof.  Jinyi Qi</strong> from UC Davis joined us for his keynote speech on "Pushing the temporal resolution of dynamic PET using multiscale information and the EXPLORER total-body PET scanner".</p>
  <p>We are offering multiple <strong>Best Paper Awards</strong> and <strong>Student Paper Awards</strong>, thanks to the support from our sponsors!<br />
  Because of this, submission deadline has been extended to <strong>August 7th</strong>.  </p>
  <h2>Scope</h2>
  <p>In the field of medical imaging, use of more than one modality (i.e. multimodal ) or more than one scale on the same target has become a growing field as more advanced techniques and devices have become available. For example, simultaneous acquisition of Positron Emission Tomography (PET) and Computed Tomography (CT) has become a standard clinical practice for a number of applications. Another example is the increasing interest in clinical diagnosis that combines high-resolution, localized pathological images and radiological images which captures disease at more global scale. Various analyses using multimodal  medical imaging and computer-aided detection systems have been developed, with the premise that additional modalities can encompass abundant information which is different and complementary to each other. While methods and tools on multiscale image analysis are not widely developed and used. Facing the growing amount of data available from multiscale multimodal  medical imaging facilities and a variety new methods for the image analysis developed so far, this MICCAI workshop aims to move forward the state of the art in multiscale multimodal medical imaging, including both algorithm development, implementation of methodology, and experimental studies. The workshop also aims to facilitate more interactions between researchers in the field of medical image analysis and the field of machine learning, especially in data fusion and multi-source learning.</p>
  <h2>Objective</h2>
  <p>MMMI aim to tackle the important challenge of dealing with medical images acquired from multiscale and multimodal imaging devices, which has been increasingly applied in research studies and clinical practice. This workshop offers an opportunity to present novel techniques and insights of multiscale multimodal  medical images analysis, as well as empirical studies involving the application of multiscale multimodal imaging for clinical use.</p>
  <h2>Topics</h2>
  Topic of submissions to the workshop include, but not limited to: <br />
  <li>Image segmentation techniques based on multiscale multimodal images</li>
  <li>Novel techniques in multiscale multimodal image acquisition</li>
  <li>Registration methods across multiscale multimodal images</li>
  <li>Fusion of images from multiple resolutions and novel visualization methods</li>
  <li>Spatial-temporal analysis using multiple modalities</li>
  <li>Fusion of image sources with different fidelities: e.g. co-analysis of EEG and fMRI</li>
  <li>Multiscale multimodal disease classification and prediction using supervised or unsupervised methods </li>
  <li>Atlas-based methods on multiple imaging modalities</li>
  <li>Cross-modality image generative methods: e.g. generation of synthetic images between CT and MR</li>
  <li>Novel radiomics methods based on multiscale multimodal imaging</li>
  <li>Shape analysis on images from multiple sources and/or multiple resolution</li>
  <li>Graph methods in medical image analysis</li>
  <li>Benchmark studies for multiscale multimodal image analysis: e.g. using electrophysiological signals for validation of fMRI data</li>
  <h2>Cooperating Organization</h2>
  <p><img src="./CCDS-Logo.png" alt="" height="50" />
  <img src="./Paige-Logo.png" alt="" height="100" /><br />
  <img src="./LNCS-Logo.jpg" alt=""  height="100" /></p>
</div>

<div id="Schedule" class="w3-container tab" style="display:none">
  <h2>Workshop Schedule</h2>
  <h3>October 13 12:30-16:30 Madrid 3</h3>
  <p>12:30-12:45 <strong>Welcome message and briefing on the multi-modal, multi-scale medical imaging analysis</strong></p>
  <p>12:45-1:30 <strong>Keynote Talk: Prof. Jinyi Qi</strong>: Pushing the temporal resolution of dynamic PET using multiscale information and the EXPLORER total-body PET scanner<br />
  <a href="https://qilab.bme.ucdavis.edu/">https://qilab.bme.ucdavis.edu/</a></p>
  <p>Jinyi Qi received his B.S. degree in Electrical Engineering in 1993 from Tsinghua University, Beijing, China, and his Ph.D. degree in Electrical Engineering in 1998 from the University of Southern California. He is now a Professor in the Department of Biomedical Engineering at UC Davis, where he served as the Interim Department Chair from 2015 to 2016. Dr. Qi has been an Associate Editor of IEEE Transactions on Medical Imaging since 2006 and is an elected Fellow of AIMBE and IEEE. His research interests include statistical image reconstruction, medical image processing, image quality evaluation, and imaging system optimization.</p>
  <p><strong>Oral Presentation</strong></p>
  <p>
  <li>1:30-1:50 O-MMMI-1: <i>Multi-Modal Image Prediction via Spatial Hybrid U-Net</i> Akib Zaman, Lu Zhang, Jingwen Yan, Dajiang Zhu</li>
  <li>1:50-2:10 O-MMMI-2: <i>Automatic Segmentation of Liver CT Image Based on Dense Pyramid Network</i> Hongli Xu, Binhua Wang</li>
  <li>2:10-2:30 O-MMMI-3: <i>OctopusNet: A Deep Learning Segmentation Network for Multi-modal Medical Images</i> Yu Chen, Jiawei Chen, Dong Wei, Yuexiang Li, Yefeng Zheng</li>
  <li>2:30-2:50 O-MMMI-4: <i>Neural Architecture Search for Optimizing Deep Belief Network Models of fMRI Data</i> Ning Qiang, Bao Ge, Qinglin Dong, Fangfei Ge, Tianming Liu</li>
  <li>2:50-3:10 O-MMMI-5: <i>Single-scan Dual-tracer Separation Network Based on Pre-trained GRU</i> Junyi Tong, Yunmei Chen, Huafeng Liu</li>
  <li>3:10-3:30 O-MMMI-6: <i>Liver Detection and Segmentation using Holistically Nested Edge Detection HED Mask RCNN</i> Supriti S Mulay, Deepika G, Jeevakala S, Keerthi Ram, Mohanasankar Sivaprakasam</li>
  <li>3:30-3:50 O-MMMI-7: <i>aEEG Signal Analysis with Ensemble Learning for Newborn Seizure Detection</i> Yini Pan, Hongfeng Li, Lili Liu, Quanzheng Li, Xinlin Hou, Bin Dong</li>
  <li>3:50-4:10 O-MMMI-8: <i>Automatic Sinus Surgery Skill Assessment Based on Instrument Segmentation and Tracking in Endoscopic Video</i> Shan Lin, Fangbo Qin, Randall Bly, Kris Moe, Blake Hannaford</li>
  </p>
  </div>

<div id="Submission" class="w3-container tab" style="display:none">
  <h2>Submission</h2>
  <h4><strong>Proceedings:</strong></h4>
  <li>The MMMI 2019 proceedings has been published as Springer <strong>Lecture Notes in Computer Science (LNCS)</strong> series. Link can be found <a href="https://www.springer.com/us/book/9783030379681">here</a></li>
  <h4><strong>Paper Formatting:</strong></h4>
  <li>Papers are limited to <strong>8 pages</strong>. Papers should be formatted in Lecture Notes in Computer Science style. Style files can be found on the <strong>Springer website</strong>. The file format for submissions is Adobe Portable Document Format (PDF). Other formats will not be accepted.</li>
  <li>Authors should consult Springer <a href="ftp://ftp.springernature.com/cs-proceeding/svproc/guidelines/Springer_Guidelines_for_Authors_of_Proceedings.pdf" target="_blank" rel="noopener">authors guidelines</a> and use their proceedings templates, either for <a href="ftp://ftp.springernature.com/cs-proceeding/llncs/llncs2e.zip" target="_blank" rel="noopener">LaTeX</a> or for <a href="ftp://ftp.springernature.com/cs-proceeding/llncs/word/splnproc1703.zip" target="_blank" rel="noopener">Word</a>, for the preparation of their papers. Springer encourages authors to include their <a href="https://goo.gl/hbsa4D" target="_blank" rel="noopener">ORCID</a> identifier in their papers. In addition, the corresponding author of each paper, acting on behalf of all of the authors of that paper, must complete and sign a Consent-to-Publish form. The corresponding author signing the copyright form should match the corresponding author marked on the paper. Once the files have been sent to Springer, changes relating to the authorship of the papers cannot be made.</li>
  <h4><strong>Blind Review:</strong></h4>
  <li>Reviewing is <u>strictly double blind</u>: authors do not know the names of the reviewers of their papers, and reviewers do not know the names of the authors. Please see the <strong>anonymity guidelines</strong> of MICCAI 2019 for detailed explanations of how to ensure this.</li>
  <h4><strong>Submission:</strong></h4>
  <li>MMMI is using an online submission system: <a href="https://cmt3.research.microsoft.com/MMMI2019">https://cmt3.research.microsoft.com/MMMI2019</a></li>
  <h4><strong>Supplemental Material</strong></h4>
  <li>Supplemental material submission is optional. This material may include: videos of results that cannot be included in the main paper, anonymized related submissions to other conferences and journals, and appendices or technical reports containing extended proofs and mathematical derivations that are not essential for understanding of the paper. Contents of the supplemental material should be referred to appropriately in the paper and that reviewers are not obliged to look at it.</li>
  <h4><strong>Simultaneous Submission</strong></h4>
  <li>Our policy is that in submitting a paper, authors implicitly acknowledge that NO paper of substantially similar content has been or will be submitted to another conference or workshop until MLMI decisions are made.</li>
</div>

<div id="Organization" class="w3-container tab" style="display:none">
  <h2>Organization</h2>
  <ui>
  <img src="./quanzheng.jpg" height="80" /><strong>Quanzheng Li</strong><br />
  Associate Professor, Department of Radiology, Harvard Medical School and Massachusetts General Hospital, Boston, MA<br />
  Email: li.quanzheng@mgh.harvard.edu<br /><br />
  </ui>
  <ui>
  <img src="./richard.jpg" height="80" /><strong>Richard Leahy</strong><br />
  Deans Professor, Electrical Engineering-Systems, Biomedical Engineering, and Radiology, University of Southern California, Los Angeles, CA<br />
  Email: leahy@sipi.usc.edu<br /><br />
  </ui>  
  <img src="./bin.jpg" height="80" /><strong>Bin Dong</strong><br />
  Associate Professor, Beijing International Center for Mathematical Research (BICMR), Peking University, Beijing, China<br />
  Email: dongbin@math.pku.edu.cn<br /><br />
  </ui>
  <ui>
  <img src="./xiang.jpg" height="80" /><strong>Xiang Li</strong><br />
  Instructor, Department of Radiology, Harvard Medical School and Massachusetts General Hospital, Boston, MA<br />
  Email: xli60@mgh.harvard.edu<br />
  </ui>  
</div>

<div id="Dates" class="w3-container tab" style="display:none">
  <h2>Dates</h2>
  <p>MMMI 2019 will be held on Oct. 13 PM, 2019, Shenzhen, China <br />
	<li>Deadline for Full Paper Submission: <strike>Midnight, Pacific Time, July 31th, 2019</strike>  Midnight, Pacific Time, August 7th, 2019</li>
	<li>Notification of Acceptance: August 15th, 2019</li>
	<li>Deadline for Camera Ready Submission: August 22th, 2019</li>
  </p>
</div>

<div id="Registration" class="w3-container tab" style="display:none">
  <h2>Registration</h2>
  <p>Please register to attend the MLMI workshop via the following MICCAI registration page: <br />
		<a href="http://www.miccai2019.org/registration/">http://www.miccai2019.org/registration/</a>
  </p>
</div>


<script>
function openTab(tabName) {
  var i;
  var x = document.getElementsByClassName("tab");
  for (i = 0; i < x.length; i++) {
    x[i].style.display = "none";  
  }
  document.getElementById(tabName).style.display = "block";  
}
</script>


</body>
</html>
